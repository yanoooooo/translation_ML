{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Encoder_Decoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanoooooo/translation_ML/blob/colab/Encoder_Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "s0C2ssdBzSHb",
        "colab_type": "code",
        "outputId": "17dde5e6-1e92-45f8-be3a-c9dbba42e843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Import Chainer \n",
        "from chainer import Chain, Variable, optimizers, serializers, datasets, training, cuda\n",
        "from chainer.training import extensions\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "from chainer import Variable\n",
        "import chainer\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1680,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "81FunFgu0pEh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Variables"
      ]
    },
    {
      "metadata": {
        "id": "a0zwhlUe0l7g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "is_debug = True\n",
        "batch_size = 2\n",
        "epoch_num = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O8oDYpC3MkXz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Models"
      ]
    },
    {
      "metadata": {
        "id": "OaqiyYxgMmk-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LyricsEncoder(chainer.Chain):\n",
        "  def __init__(self, vocab_size):\n",
        "    super(LyricsEncoder, self).__init__()\n",
        "    with self.init_scope():\n",
        "      self.embed = L.EmbedID(vocab_size, 8)\n",
        "      self.lstm1 = L.LSTM(None, 32)\n",
        "      self.l1= L.Linear(None, 16)\n",
        "      \n",
        "  def reset_state():\n",
        "    self.lstm1.reset_state()\n",
        "      \n",
        "  def forward(self, x_data):\n",
        "    x = Variable(cuda.to_gpu(x_data))\n",
        "#     print('x: ', x.shape)\n",
        "    h = F.tanh(self.embed(x))\n",
        "#     print('a: ', h.shape)\n",
        "    h = self.lstm1(h)\n",
        "#     print('b: ', h.shape)\n",
        "    h = self.l1(h)\n",
        "#     print('c: ', h.shape)\n",
        "    \n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jBXcIIRwmQUs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MoraEncoder(chainer.Chain):\n",
        "  def __init__(self, vocab_size):\n",
        "    super(MoraEncoder, self).__init__()\n",
        "    with self.init_scope():\n",
        "      self.embed = L.EmbedID(vocab_size, 8)\n",
        "      self.lstm1 = L.LSTM(None, 32)\n",
        "      self.l1= L.Linear(None, 16)\n",
        "      \n",
        "  def reset_state():\n",
        "    self.lstm1.reset_state()\n",
        "      \n",
        "  def forward(self, x_data):\n",
        "    x = Variable(cuda.to_gpu(x_data))\n",
        "    h = F.tanh(self.embed(x))\n",
        "    h = self.lstm1(h)\n",
        "    h = self.l1(h)\n",
        "    \n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PsaL7wVUmQji",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LyricsDecoder(chainer.Chain):\n",
        "  def __init__(self, vocab_size):\n",
        "    super(LyricsDecoder, self).__init__()\n",
        "    with self.init_scope():\n",
        "      self.embed = L.EmbedID(vocab_size, 16)\n",
        "      self.lstm1 = L.LSTM(None, 32)\n",
        "      self.l1= L.Linear(None, vocab_size)\n",
        "      \n",
        "  def reset_state():\n",
        "    self.lstm1.reset_state()\n",
        "      \n",
        "  def forward(self, h, t_data):\n",
        "    t = Variable(cuda.to_gpu(t_data))\n",
        "#     print('t: ', t.shape)\n",
        "    h = self.lstm1(h)\n",
        "#     print('h: ', h.shape)\n",
        "    y = self.l1(h)\n",
        "#     print('y: ', y.shape)\n",
        "    \n",
        "#     print(y)\n",
        "#     print(np.argmax(y.data, axis=1))\n",
        "#     print(t)\n",
        "#     print(np.argmax(t.data))\n",
        "    return F.softmax_cross_entropy(y, t), F.accuracy(y, t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Io7KFzsbmQuR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MoraDecoder(chainer.Chain):\n",
        "  def __init__(self, vocab_size):\n",
        "    super(MoraDecoder, self).__init__()\n",
        "    with self.init_scope():\n",
        "      self.embed = L.EmbedID(vocab_size, 8)\n",
        "      self.lstm1 = L.LSTM(None, 32)\n",
        "      self.l1= L.Linear(None, vocab_size)\n",
        "      \n",
        "  def reset_state():\n",
        "    self.lstm1.reset_state()\n",
        "      \n",
        "  def forward(self, h, t_data):\n",
        "    t = Variable(cuda.to_gpu(t_data))\n",
        "#     h = F.tanh(self.embed(x))\n",
        "    h = self.lstm1(h)\n",
        "    y = self.l1(h)\n",
        "    \n",
        "    return F.softmax_cross_entropy(y, t), F.accuracy(y, t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DFVQ7Qtec-Sm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Data Utils"
      ]
    },
    {
      "metadata": {
        "id": "byTpvf2vdB3Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_word_list(data):\n",
        "  result = {}\n",
        "  for i in range(len(data)):\n",
        "    lt = data[i].split()\n",
        "#     data[i] = lt\n",
        "    for w in lt:\n",
        "      if w not in result:\n",
        "        result[w] = len(result)\n",
        "  result['<eos>'] = len(result)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aAIuQUUukKUk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_split_list(data):\n",
        "  result = []\n",
        "  for i in range(len(data)):\n",
        "    lt = data[i].split()\n",
        "    if(len(lt) > 0 and not 'None' in lt):\n",
        "      lt.append('<eos>')\n",
        "    else:\n",
        "      lt = []\n",
        "    result.append(lt)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Xrc0Vk8seTl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batch(data, batch_size):\n",
        "  result = []\n",
        "  for index in range(0, math.ceil(len(data) / batch_size)):\n",
        "    tune = []\n",
        "    batch = data[index*batch_size:min([index * batch_size + batch_size, len(data)])]\n",
        "    max_length = max([len(st) for st in batch])\n",
        "    tune_flg = False\n",
        "    for j in range(max_length):\n",
        "      phrase = []\n",
        "      for s in batch:\n",
        "        # 曲の区切りには空の配列を挿入\n",
        "        if len(s) == 0:\n",
        "          tune_flg = True\n",
        "          continue\n",
        "        if j < len(s):\n",
        "          phrase.append(s[j])\n",
        "        else:\n",
        "          phrase.append(-1)\n",
        "      tune.append(phrase)\n",
        "      # tune.append([s[j] if j < len(s) else None for s in batch])\n",
        "    if tune_flg:\n",
        "        result.append([])\n",
        "    result.append(tune)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eukDuVr012q2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_zeros_vector(batch, dict):\n",
        "  result = []\n",
        "  # 入力文章数、単語数の0ベクトル配列を作成\n",
        "  for i in range(0, len(batch)):\n",
        "    xi = np.zeros((len(batch[i]), len(dict)), np.int32)\n",
        "    for k, word in enumerate((batch[i])):\n",
        "      if word is not None: xi[k, dict[word]] = 1\n",
        "    result.append(xi)\n",
        "  return np.array(result, dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BbZ8T8Kh9P7w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_target_labels(batch, dict):\n",
        "  result = []\n",
        "  # 入力文章数、単語数の0ベクトル配列を作成\n",
        "  for i in range(0, len(batch)):\n",
        "    xi = []\n",
        "    for k, word in enumerate((batch[i])):\n",
        "      if word is not None: xi.append(dict[word])\n",
        "    result.append(xi)\n",
        "  return np.array(result, dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jCPQxjjE01gT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Trainning Section"
      ]
    },
    {
      "metadata": {
        "id": "5FMt8U390x7J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(ja_file_path, en_file_path):\n",
        "  ja_data = pd.read_csv(ja_file_path)\n",
        "  en_data = pd.read_csv(en_file_path)\n",
        "  \n",
        "  # 学習に必要なリストの取得\n",
        "  lyrics_list = {'en': get_split_list(en_data['Lyrics']), 'ja': get_split_list(ja_data['Lyrics'])}\n",
        "  mora_list = {'en': get_split_list(en_data['Mora']), 'ja': get_split_list(ja_data['Mora'])}\n",
        "  lyrics_vocab_list = {'en': get_word_list(en_data['Lyrics']), 'ja': get_word_list(ja_data['Lyrics'])}\n",
        "  mora_vocab_list = {'en': get_word_list(en_data['Mora']), 'ja': get_word_list(ja_data['Mora'])}\n",
        "  \n",
        "  # モデルの作成\n",
        "  lyrics_encoder = LyricsEncoder(len(lyrics_vocab_list['en']))\n",
        "  mora_encoder = MoraEncoder(len(mora_vocab_list['en']))\n",
        "  lyrics_decoder = LyricsDecoder(len(lyrics_vocab_list['ja']))\n",
        "  mora_decoder = MoraDecoder(len(mora_vocab_list['ja']))\n",
        "  \n",
        "  # gpuの使用\n",
        "  lyrics_encoder.to_gpu()\n",
        "  mora_encoder.to_gpu()\n",
        "  lyrics_decoder.to_gpu()\n",
        "  mora_decoder.to_gpu()\n",
        "  \n",
        "  # optimizerの作成\n",
        "  optimizers_list = {\n",
        "      'en_lyrics': optimizers.Adam().setup(lyrics_encoder),\n",
        "      'en_mora': optimizers.Adam().setup(mora_encoder),\n",
        "      'ja_lyric': optimizers.Adam().setup(lyrics_decoder),\n",
        "      'ja_mora': optimizers.Adam().setup(mora_decoder),\n",
        "  }\n",
        "  \n",
        "  for epoch in range(0, epoch_num):\n",
        "    print('epoch: ', epoch+1)\n",
        "    l_sum_accuracy, l_sum_loss, l_sum_batch_size = 0, 0, 0\n",
        "    m_sum_accuracy, m_sum_loss, m_sum_batch_size = 0, 0, 0\n",
        "    # 曲が違う場合はリセットしなければならない\n",
        "#     lyrics_encoder.reset_state()\n",
        "#     mora_encoder.reset_state()\n",
        "\n",
        "    # create batch\n",
        "    en_lyrics_batch = get_batch(lyrics_list['en'], batch_size)\n",
        "    en_mora_batch = get_batch(mora_list['en'], batch_size)\n",
        "    ja_lyrics_batch = get_batch(lyrics_list['ja'], batch_size)\n",
        "    ja_mora_batch = get_batch(mora_list['ja'], batch_size)\n",
        "    \n",
        "    for i, (x_l_batch, x_m_batch, t_l_batch, t_m_batch) in enumerate(zip(en_lyrics_batch, en_mora_batch, ja_lyrics_batch, ja_mora_batch)):\n",
        "      # ------- Encoder ------\n",
        "#       print(x_l_batch)\n",
        "      x_l_batch = get_zeros_vector(x_l_batch, lyrics_vocab_list['en'])\n",
        "      x_m_batch = get_zeros_vector(x_m_batch, mora_vocab_list['en'])\n",
        "      t_l_batch = get_target_labels(t_l_batch, lyrics_vocab_list['ja'])\n",
        "      t_m_batch = get_target_labels(t_m_batch, mora_vocab_list['ja'])\n",
        "      \n",
        "      for x_l, x_m, t_l, t_m in zip(x_l_batch, x_m_batch, t_l_batch, t_m_batch):\n",
        "        lyrics_y = lyrics_encoder.forward(x_l)\n",
        "        mora_y = mora_encoder.forward(x_m)\n",
        "    \n",
        "        y = F.tanh(lyrics_y + mora_y)\n",
        "      \n",
        "        l_loss, l_acc = lyrics_decoder.forward(y, t_l)\n",
        "        m_loss, m_acc = mora_decoder.forward(y, t_m)\n",
        "        \n",
        "        \n",
        "        lyrics_encoder.cleargrads()\n",
        "        mora_encoder.cleargrads()\n",
        "        lyrics_decoder.cleargrads()\n",
        "        mora_decoder.cleargrads()\n",
        "        l_loss.backward()\n",
        "        l_loss.unchain_backward()\n",
        "        m_loss.backward()\n",
        "        m_loss.unchain_backward()\n",
        "        \n",
        "        for optimizer in optimizers_list.values():\n",
        "          optimizer.update()\n",
        "          \n",
        "        l_sum_loss += float(l_loss.data) * len(x_l)\n",
        "        l_sum_accuracy += float(l_acc.data) * len(x_l)\n",
        "        l_sum_batch_size += len(x_l)\n",
        "        m_sum_loss += float(m_loss.data) * len(x_m)\n",
        "        m_sum_accuracy += float(m_acc.data) * len(x_m)\n",
        "        m_sum_batch_size += len(x_m)\n",
        "\n",
        "    # show training data loss and accuracy\n",
        "    l_loss = l_sum_loss / l_sum_batch_size\n",
        "    l_accuracy = l_sum_accuracy / l_sum_batch_size\n",
        "    m_loss = m_sum_loss / m_sum_batch_size\n",
        "    m_accuracy = m_sum_accuracy / m_sum_batch_size\n",
        "\n",
        "    print('train lyrics mean loss={}, accuracy={}'.format(l_loss, l_accuracy))\n",
        "    print('train mora mean loss={}, accuracy={}'.format(m_loss, m_accuracy))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DZWYfwgrzdRn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Main Function"
      ]
    },
    {
      "metadata": {
        "id": "KOSk3SURzSGv",
        "colab_type": "code",
        "outputId": "77d7e880-e5b4-409c-d3ae-5d79e4795d9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "  if is_debug:\n",
        "    ja_file_path = '/gdrive/My Drive/Research/LyricsTranslation/NeuralNetwork/translation_ML/data/ja.csv'\n",
        "    en_file_path = '/gdrive/My Drive/Research/LyricsTranslation/NeuralNetwork/translation_ML/data/en.csv'\n",
        "  else:\n",
        "    file_path = '/gdrive/My Drive/DeepLearning/GestureRecognition/data/formatted/train_classification.csv'\n",
        "\n",
        "  train(ja_file_path, en_file_path)\n",
        "  \n",
        "  print('Done!')"
      ],
      "execution_count": 1692,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  1\n",
            "train lyrics mean loss=2.556262819390548, accuracy=0.0\n",
            "train mora mean loss=1.5360374701650519, accuracy=0.0\n",
            "epoch:  2\n",
            "train lyrics mean loss=2.437440972579153, accuracy=0.15789473684210525\n",
            "train mora mean loss=1.349625593737552, accuracy=0.42105263157894735\n",
            "epoch:  3\n",
            "train lyrics mean loss=2.4011656485105815, accuracy=0.21052631578947367\n",
            "train mora mean loss=1.2218687973524396, accuracy=0.47368421052631576\n",
            "epoch:  4\n",
            "train lyrics mean loss=2.3894752954181873, accuracy=0.15789473684210525\n",
            "train mora mean loss=1.1744432794420343, accuracy=0.47368421052631576\n",
            "epoch:  5\n",
            "train lyrics mean loss=2.392615054783068, accuracy=0.15789473684210525\n",
            "train mora mean loss=1.143401114564193, accuracy=0.47368421052631576\n",
            "epoch:  6\n",
            "train lyrics mean loss=2.3926997435720345, accuracy=0.15789473684210525\n",
            "train mora mean loss=1.1175746729499416, accuracy=0.47368421052631576\n",
            "epoch:  7\n",
            "train lyrics mean loss=2.38263884343599, accuracy=0.15789473684210525\n",
            "train mora mean loss=1.0883429646492004, accuracy=0.47368421052631576\n",
            "epoch:  8\n",
            "train lyrics mean loss=2.3637037151738216, accuracy=0.15789473684210525\n",
            "train mora mean loss=1.0647402311626233, accuracy=0.47368421052631576\n",
            "epoch:  9\n",
            "train lyrics mean loss=2.339668299022474, accuracy=0.15789473684210525\n",
            "train mora mean loss=1.0552734669886137, accuracy=0.47368421052631576\n",
            "epoch:  10\n",
            "train lyrics mean loss=2.315114460493389, accuracy=0.15789473684210525\n",
            "train mora mean loss=1.0528452239538495, accuracy=0.47368421052631576\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}